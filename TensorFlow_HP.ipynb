{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk import tokenize\n",
    "\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "def getcorpus_local(local):\n",
    "    with open(str(local)) as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\", exclude_encodings=[\"iso-8859-7\"])\n",
    "        words = soup.get_text()\n",
    "        #words= re.sub(\"[^a-zA-Z' ]+\", \" \", words)\n",
    "        words = re.sub(\"\\\\n\",\"\",words)\n",
    "        words =re.sub(\"\\\\'\", \"\", words)\n",
    "        return words\n",
    "    \n",
    "HPOne = getcorpus_local(R\"C:\\Users\\heather\\Desktop\\Post Kaiser Projects\\Harry Potter\\Harry Potter and the Sorcerer.TXT\")\n",
    "\n",
    "HPOne = HPOne[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "#\n",
    "#                                           Import all the things!\n",
    "#\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "#\n",
    "#                                           Get data\n",
    "#\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 433563 characters\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################################################\n",
    "#\n",
    "#\n",
    "#                                           Process text\n",
    "#\n",
    "#\n",
    "##############################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read, then decode for py2 compat.\n",
    "#text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "#text =open(r\"C:\\Users\\heather\\Desktop\\Post Kaiser Projects\\Harry Potter\\Harry Potter and the Sorcerer.txt\", 'rb').read().strip()\n",
    "text = HPOne\n",
    "\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerers Stone CHAPTER ONE THE BOY WHO LIVED Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youd expect to be involved in any\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ' ' :   0,\n",
      "  '!' :   1,\n",
      "  '\"' :   2,\n",
      "  '(' :   3,\n",
      "  ')' :   4,\n",
      "  '*' :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '0' :   9,\n",
      "  '1' :  10,\n",
      "  '2' :  11,\n",
      "  '3' :  12,\n",
      "  '4' :  13,\n",
      "  '5' :  14,\n",
      "  '6' :  15,\n",
      "  '7' :  16,\n",
      "  '8' :  17,\n",
      "  '9' :  18,\n",
      "  ':' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Harry Potter ' ---- characters mapped to int ---- > [29 49 66 66 73  0 37 63 68 68 53 66  0]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "a\n",
      "r\n",
      "r\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Harry Potter and the Sorcerers Stone CHAPTER ONE THE BOY WHO LIVED Mr. and Mrs. Dursley, of number fo'\n",
      "'ur, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were t'\n",
      "'he last people youd expect to be involved in anything strange or mysterious, because they just didnt '\n",
      "'hold with such nonsense. Mr. Dursley was the director of a firm called Grunnings, which made drills. '\n",
      "'He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursle'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'Harry Potter and the Sorcerers Stone CHAPTER ONE THE BOY WHO LIVED Mr. and Mrs. Dursley, of number f'\n",
      "Target data: 'arry Potter and the Sorcerers Stone CHAPTER ONE THE BOY WHO LIVED Mr. and Mrs. Dursley, of number fo'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 29 ('H')\n",
      "  expected output: 49 ('a')\n",
      "Step    1\n",
      "  input: 49 ('a')\n",
      "  expected output: 66 ('r')\n",
      "Step    2\n",
      "  input: 66 ('r')\n",
      "  expected output: 66 ('r')\n",
      "Step    3\n",
      "  input: 66 ('r')\n",
      "  expected output: 73 ('y')\n",
      "Step    4\n",
      "  input: 73 ('y')\n",
      "  expected output: 0 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 80) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           20480     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 80)            82000     \n",
      "=================================================================\n",
      "Total params: 4,040,784\n",
      "Trainable params: 4,040,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 70, 28, 26, 55, 30,  9,  6, 76, 73,  5,  4, 37, 19, 58, 58, 11,\n",
       "       26, 53,  8, 39, 52, 52, 42, 66, 50, 20, 36, 35, 43, 69,  2,  4,  6,\n",
       "       33, 15, 29, 22, 43,  7, 21, 60, 73, 68, 27, 49, 54, 47,  4, 32, 73,\n",
       "       70,  8, 65, 70, 10, 57, 34, 46, 22, 39, 12, 22, 66, 67, 48, 55, 13,\n",
       "       59, 72, 74, 66,  9, 57, 67, 48, 55, 50, 70, 70, 46, 46, 57, 73, 35,\n",
       "        0, 21,  2, 48, 30, 28,  4, 25, 36, 43, 23, 46, 67, 77, 23],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'ad in his whole life -- more money than even Dudley had ever had. \"Might as well get yer uniform,\" s'\n",
      "\n",
      "Next Char Predictions: \n",
      " '9vGEgI0,ây*)P:jj2Ee.RddUrb;ONVu\"),L6HAV-?lytFafZ)Kyv.qv1iMYAR3Ars\\\\g4kxzr0is\\\\gbvvYYiyN ?\"\\\\IG)DOVBYsœB'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 80)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.382399\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "67/67 [==============================] - 139s 2s/step - loss: 3.2449\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 136s 2s/step - loss: 2.3141\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 2.0992\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 1.9131\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 1.7595\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 147s 2s/step - loss: 1.6320\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 145s 2s/step - loss: 1.5288\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 151s 2s/step - loss: 1.4481\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 145s 2s/step - loss: 1.3799\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 150s 2s/step - loss: 1.3239\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.2757\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 159s 2s/step - loss: 1.2323\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 165s 2s/step - loss: 1.1903\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 155s 2s/step - loss: 1.1495\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 163s 2s/step - loss: 1.1117\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 1.0723\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 132s 2s/step - loss: 1.0353\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 141s 2s/step - loss: 0.9950\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 148s 2s/step - loss: 0.9553\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 0.9128\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 132s 2s/step - loss: 0.8722\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.8308\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.7859\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.7418\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 0.6999\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 132s 2s/step - loss: 0.6594\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 0.6170\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 136s 2s/step - loss: 0.5798\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 136s 2s/step - loss: 0.5435\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 135s 2s/step - loss: 0.5138\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 0.4848\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.4572\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.4321\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 143s 2s/step - loss: 0.4156\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 134s 2s/step - loss: 0.3955\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 151s 2s/step - loss: 0.3813\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 145s 2s/step - loss: 0.3701\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 151s 2s/step - loss: 0.3560\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 144s 2s/step - loss: 0.3470\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 144s 2s/step - loss: 0.3399\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 142s 2s/step - loss: 0.3308\n",
      "Epoch 42/100\n",
      "67/67 [==============================] - 135s 2s/step - loss: 0.3237\n",
      "Epoch 43/100\n",
      "67/67 [==============================] - 138s 2s/step - loss: 0.3175\n",
      "Epoch 44/100\n",
      "67/67 [==============================] - 136s 2s/step - loss: 0.3117\n",
      "Epoch 45/100\n",
      "67/67 [==============================] - 141s 2s/step - loss: 0.3048\n",
      "Epoch 46/100\n",
      "67/67 [==============================] - 138s 2s/step - loss: 0.3042\n",
      "Epoch 47/100\n",
      "67/67 [==============================] - 141s 2s/step - loss: 0.2991\n",
      "Epoch 48/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.2961\n",
      "Epoch 49/100\n",
      "67/67 [==============================] - 140s 2s/step - loss: 0.2930\n",
      "Epoch 50/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.2906\n",
      "Epoch 51/100\n",
      "67/67 [==============================] - 136s 2s/step - loss: 0.2903\n",
      "Epoch 52/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.2850\n",
      "Epoch 53/100\n",
      "67/67 [==============================] - 145s 2s/step - loss: 0.2839\n",
      "Epoch 54/100\n",
      "67/67 [==============================] - 145s 2s/step - loss: 0.2846\n",
      "Epoch 55/100\n",
      "67/67 [==============================] - 155s 2s/step - loss: 0.2835\n",
      "Epoch 56/100\n",
      "67/67 [==============================] - 138s 2s/step - loss: 0.2809\n",
      "Epoch 57/100\n",
      "67/67 [==============================] - 140s 2s/step - loss: 0.2801\n",
      "Epoch 58/100\n",
      "67/67 [==============================] - 154s 2s/step - loss: 0.2768\n",
      "Epoch 59/100\n",
      "67/67 [==============================] - 177s 3s/step - loss: 0.2763\n",
      "Epoch 60/100\n",
      "67/67 [==============================] - 184s 3s/step - loss: 0.2768\n",
      "Epoch 61/100\n",
      "67/67 [==============================] - 163s 2s/step - loss: 0.2764\n",
      "Epoch 62/100\n",
      "67/67 [==============================] - 169s 3s/step - loss: 0.2748\n",
      "Epoch 63/100\n",
      "67/67 [==============================] - 168s 3s/step - loss: 0.2754\n",
      "Epoch 64/100\n",
      "67/67 [==============================] - 169s 3s/step - loss: 0.2743\n",
      "Epoch 65/100\n",
      "67/67 [==============================] - 182s 3s/step - loss: 0.2739\n",
      "Epoch 66/100\n",
      "67/67 [==============================] - 182s 3s/step - loss: 0.2753\n",
      "Epoch 67/100\n",
      "67/67 [==============================] - 170s 3s/step - loss: 0.2735\n",
      "Epoch 68/100\n",
      "67/67 [==============================] - 161s 2s/step - loss: 0.2736\n",
      "Epoch 69/100\n",
      "67/67 [==============================] - 162s 2s/step - loss: 0.2740\n",
      "Epoch 70/100\n",
      "67/67 [==============================] - 144s 2s/step - loss: 0.2731\n",
      "Epoch 71/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.2707\n",
      "Epoch 72/100\n",
      "67/67 [==============================] - 138s 2s/step - loss: 0.2720\n",
      "Epoch 73/100\n",
      "67/67 [==============================] - 157s 2s/step - loss: 0.2720\n",
      "Epoch 74/100\n",
      "67/67 [==============================] - 137s 2s/step - loss: 0.2733\n",
      "Epoch 75/100\n",
      "67/67 [==============================] - 159s 2s/step - loss: 0.2690\n",
      "Epoch 76/100\n",
      "67/67 [==============================] - 160s 2s/step - loss: 0.2692\n",
      "Epoch 77/100\n",
      "67/67 [==============================] - 133s 2s/step - loss: 0.2692\n",
      "Epoch 78/100\n",
      "67/67 [==============================] - 135s 2s/step - loss: 0.2684\n",
      "Epoch 79/100\n",
      "67/67 [==============================] - 152s 2s/step - loss: 0.2678\n",
      "Epoch 80/100\n",
      "67/67 [==============================] - 193s 3s/step - loss: 0.2695\n",
      "Epoch 81/100\n",
      "67/67 [==============================] - 176s 3s/step - loss: 0.2720\n",
      "Epoch 82/100\n",
      "67/67 [==============================] - 187s 3s/step - loss: 0.2730\n",
      "Epoch 83/100\n",
      "67/67 [==============================] - 163s 2s/step - loss: 0.2747\n",
      "Epoch 84/100\n",
      "67/67 [==============================] - 168s 3s/step - loss: 0.2743\n",
      "Epoch 85/100\n",
      "67/67 [==============================] - 164s 2s/step - loss: 0.2731\n",
      "Epoch 86/100\n",
      "67/67 [==============================] - 141s 2s/step - loss: 0.2747\n",
      "Epoch 87/100\n",
      "67/67 [==============================] - 142s 2s/step - loss: 0.2758\n",
      "Epoch 88/100\n",
      "67/67 [==============================] - 141s 2s/step - loss: 0.2765\n",
      "Epoch 89/100\n",
      "67/67 [==============================] - 140s 2s/step - loss: 0.2772\n",
      "Epoch 90/100\n",
      "67/67 [==============================] - 149s 2s/step - loss: 0.2783\n",
      "Epoch 91/100\n",
      "67/67 [==============================] - 151s 2s/step - loss: 0.2784\n",
      "Epoch 92/100\n",
      "67/67 [==============================] - 154s 2s/step - loss: 0.2789\n",
      "Epoch 93/100\n",
      "67/67 [==============================] - 157s 2s/step - loss: 0.2798\n",
      "Epoch 94/100\n",
      "67/67 [==============================] - 186s 3s/step - loss: 0.2802\n",
      "Epoch 95/100\n",
      "67/67 [==============================] - 172s 3s/step - loss: 0.2813\n",
      "Epoch 96/100\n",
      "67/67 [==============================] - 148s 2s/step - loss: 0.2788\n",
      "Epoch 97/100\n",
      "67/67 [==============================] - 159s 2s/step - loss: 0.2794\n",
      "Epoch 98/100\n",
      "67/67 [==============================] - 186s 3s/step - loss: 0.2780\n",
      "Epoch 99/100\n",
      "67/67 [==============================] - 179s 3s/step - loss: 0.2808\n",
      "Epoch 100/100\n",
      "67/67 [==============================] - 144s 2s/step - loss: 0.2821\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_100'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            20480     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 80)             82000     \n",
      "=================================================================\n",
      "Total params: 4,040,784\n",
      "Trainable params: 4,040,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "owl to die.\" \"Th the best team were cheering. What they were rest of the first years. Even Neville thought of Hagrid, expelled but peture was a small voice in a low, with relatives -- George get to Professor Dumbledore and Hermione read the cuptofth matter if yeh know wouldnt be sure I dont know...\" said Ron finally. \"Flamels juct go somewhe points from Gryffindor.\" \"Hes just made that rule up,\" Harry muttered an rup a lot -- great with animals.\" Harry wondered if Hagrid had taken Harry aside. \"Dont put- on Saturday,\" said Ron. \"Just come proveled into a gige, but Snape tried to robsame of such a telephine pain in his truth concerent wild eyes on all the times Dudley had punched him on the very last thing they werent surprised straper little reasons were delivered to Harry. He looked voice. \"Then Im going to do magic.\" Harry croaked, \"that was what happened when Vol-, sorry -- I mean, the name of just be offered for a Snitch teams and something reflected because his ron. Harry walked more \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"owl \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue and hat much lo;er, too -- well -- noble to use them.\" \"Its lucky I got it to kill mention, Dursley and Malfoy darking behind his make a giant called after him to hurry up, I cant breathe!\" Harry gave a sloud, but not expection,\" Harry heard Hermione snap. \"Its Wing-gar-dium Levi-o-sa, tas no snapes leston. It took reaching the twins. \"Oh, him -- Im werent you, and up in the air behind him. If she was mean an --\" Quirrell stood up. \"So -- about that the exams werent ride packed with Muggles?\" \"If thats water in my poss Snatch. Binns had gone. He flew in cabe hed rather r to block the unicorns -- I could have been crossbow, and all the way up the street that someone got to go andward, but it was still looking forward to the house. He was still determined not to meet his family and sad been co they shouted, as though this was as horrible thought to look in front. reat, who had been wanting. The way to his forehead with an orner the high chair. Non he had perfoy to come bursting through t\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"blue \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
